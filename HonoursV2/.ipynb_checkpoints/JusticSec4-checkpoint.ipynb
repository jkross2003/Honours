{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d106ed6-88ee-4de5-87d7-7e04c7341b73",
   "metadata": {},
   "source": [
    "# 4. Optimising Post-Merger Detection Rate \n",
    "We define the optimal detector as the detector with the highest detection rate of post-merger signals. A detection is counted as an SNR > 5 which is plausible in the literature (see pp. 10 of [Dietrich et al. 2020] for further details). SNR is calculated using bilby by injecting NR (strain) waveforms of BNS mergers (cropped to the post-merger phase) into each possible detector. The detector response depends on the injection parameters e.g. distance from merger event SNR $\\propto 1/d$. All detection rates below are for single detectors i.e. not in a network! \n",
    "**Instructions**: Use the notebook ``ResultsSupplementary.ipynb`` to setup the CoRE database and ensure Mallika's code is in the parent directory. Also ensure that bilby is version 1.1.3! (newer versions will require NS masses in injection parameters)\n",
    "\n",
    "**TODO**: \n",
    "\n",
    "1. Explore alternative figure of merit(s) for possible detector designs. For example, a simple extension would be to maximise the detection rate *in a network* with other interferometric detectors.  \n",
    "\n",
    "2. A lot of the functions in the ``BNS_Optimisation_Module_Randomised`` module could be rewritten using the module watpy recommended by the authors of the CoRe database to future-proof the code.  \n",
    "\n",
    "3. Correct waveform scaling to make use of the full waveform set available.\n",
    "\n",
    "4. Calculate detection rate in a network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d09e2b-ccc4-44a4-bfe2-adefd74a9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finesse\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import nemo_optimisation_modules as nom\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d9555-41e9-452f-bb4c-9137242a3817",
   "metadata": {},
   "source": [
    "## 4.1 Generating Sensitivity Curves  \n",
    "The explored parameters are: common-mode tuning ($\\phi_\\text{comm}$), differential-mode tuning ($\\phi_\\text{diff}$), SRC length ($L_\\text{SRC}$), ITM transmittivity ($T_\\text{ITM}$). Parameter intervals are stored in variables of the form e.g. ``vary_phiComm``and are logarithmically spaced and divided into 10 values i.e. $10^4$ detector configurations. The function ``Finesse_sensitivity_into_txt`` in ``nemo_optimisation_modules`` is used to export the ASD (qnoise) curve for each detector configuration. Each curve has the same logarithmic frequency scale $100\\text{Hz}\\leq f\\leq10\\text{kHz}$. We include 7dB of squeezing. The naming convention for the text files is: ``vSRM_i,j,k,l_phiComm_phiDiff_srcL_itmT_prmT_lasPow_ASD_with_RP.txt`` (``i,j,k,l`` are indices for parameter intervals).\n",
    "\n",
    "**Instructions**: Before running, create a folder in the parent directory and replace the ``curves_folder`` variable respectively (text files are stored here).\n",
    "\n",
    "**TODO**: Rerun the sensitivity curves with smaller intervals surrounding the optimal design in Section 4.3 with higher resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9512bfbb-ec1e-4c00-9fa8-75da80a4dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nemo_optimisation_modules as nom\n",
    "# import numpy as np\n",
    "# import time as t\n",
    "\n",
    "# vary_phiComm = np.geomspace(1e-1, 180, 10)\n",
    "# vary_phiDiff = np.geomspace(1e-1, 180, 10)\n",
    "# vary_srcL = np.geomspace(10, 1e3, 10)\n",
    "# vary_itmT = np.geomspace(1e-2, 0.5, 10)\n",
    "\n",
    "# changed_itmT = False\n",
    "# store_prmT = 0\n",
    "# store_lasPow = 0\n",
    "# # curves_folder\n",
    "\n",
    "# start_time = t.time()\n",
    "# for i, itmT in enumerate(vary_itmT):\n",
    "#     changed_itmT = True\n",
    "#     for j, srcL in enumerate(vary_srcL):\n",
    "#         for k, phiDiff in enumerate(vary_phiDiff):\n",
    "#             for l, phiComm in enumerate(vary_phiComm):\n",
    "#                 if changed_itmT:\n",
    "#                     print('itmT changed')\n",
    "#                     prmT, lasPow = nom.Finesse_sensitivity_into_txt(params_idx=[i,j,k,l],save_path=\"./FinalSensitivityCurvesSqueezed/\",phiComm=phiComm,phiDiff=phiDiff,srcL=srcL,itmT=itmT,prmT=0,lasPow=0,optimise_prmT=True,squeezing=True)\n",
    "#                     store_prmT = prmT\n",
    "#                     store_lasPow = lasPow\n",
    "#                     changed_itmT = False\n",
    "#                 else:\n",
    "#                     nom.Finesse_sensitivity_into_txt(params_idx=[i,j,k,l],save_path=\"./FinalSensitivityCurvesSqueezed/\",phiComm=phiComm,phiDiff=phiDiff,srcL=srcL,itmT=itmT,prmT=store_prmT,lasPow=store_lasPow,optimise_prmT=False,squeezing=True)\n",
    "# print(t.time() - start_time)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afa398-85dd-4cac-8f9a-126ec52a7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "vary_phiComm = np.geomspace(1e-1, 180, 10)\n",
    "vary_phiDiff = np.geomspace(1e-1, 180, 10)\n",
    "vary_srcL = np.geomspace(10, 1e3, 10)\n",
    "vary_itmT = np.geomspace(1e-2, 0.5, 10)\n",
    "\n",
    "def run_sensitivity_analysis(i, itmT):\n",
    "    store_prmT, store_lasPow = 0, 0\n",
    "    changed_itmT = True\n",
    "    for srcL in vary_srcL:\n",
    "        for phiDiff in vary_phiDiff:\n",
    "            for phiComm in vary_phiComm:\n",
    "                if changed_itmT:\n",
    "                    # Optimize only when itmT changes\n",
    "                    prmT, lasPow = nom.Finesse_sensitivity_into_txt(\n",
    "                        params_idx=[i],\n",
    "                        save_path=\"./FinalSensitivityCurvesSqueezed/\",\n",
    "                        phiComm=phiComm,\n",
    "                        phiDiff=phiDiff,\n",
    "                        srcL=srcL,\n",
    "                        itmT=itmT,\n",
    "                        prmT=0,\n",
    "                        lasPow=0,\n",
    "                        optimise_prmT=True,\n",
    "                        squeezing=True\n",
    "                    )\n",
    "                    store_prmT, store_lasPow = prmT, lasPow\n",
    "                    changed_itmT = False\n",
    "                else:\n",
    "                    nom.Finesse_sensitivity_into_txt(\n",
    "                        params_idx=[i],\n",
    "                        save_path=\"./FinalSensitivityCurvesSqueezed/\",\n",
    "                        phiComm=phiComm,\n",
    "                        phiDiff=phiDiff,\n",
    "                        srcL=srcL,\n",
    "                        itmT=itmT,\n",
    "                        prmT=store_prmT,\n",
    "                        lasPow=store_lasPow,\n",
    "                        optimise_prmT=False,\n",
    "                        squeezing=True\n",
    "                    )\n",
    "\n",
    "# Start parallel processing\n",
    "start_time = t.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(run_sensitivity_analysis, range(len(vary_itmT)), vary_itmT)\n",
    "print(f\"Time taken: {t.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5c2e2c-de23-4dc9-a678-7d2563176540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vary_itmT)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa9be9-9079-44f0-a29b-3413408820bb",
   "metadata": {},
   "source": [
    "## 4.2 Search for Optimal Design\n",
    "See Mallika's report for more details (and acknowledgements for using/adapting their code)! Here is a brief explanation (more for my own clarity, but hopefully the reader finds it helpful). For each of the 10,000 detector designs (ASD curves generated in Section 4.1 and saved in text files), the function ``IFOmakerFromASDArray`` in ``BNS_Optimisation_Module_Randomised`` is used to create an Interferometer object in bilby. We take the NR waveform set (indexed by the variable ``index``) and duplicate it ``repeat_waveforms`` times i.e. we have ``len(index)*repeat_waveforms`` total BNS merger events. We simulate an *observing run* of these BNS merger events randomly distributed in space up to a detector horizon (``detector_horizon`` by default is 400 Mpc c.f. 100-200 Mpc for LIGO) over an *observing time* by randomising ``injection parameters`` used by bilby to calculate the detector response. The function ``calculate_SNR`` in ``BNS_Optimisation_Module_Randomised`` calculates the number of detections during the observing run (SNR > 5) and the ``detection rate`` is normalised to units of $\\text{yr}^{-1}$. A key parameter here is the ``assumed`` BNS merger rate! (``merger_rate`` has units of $\\text{Gpc}^{-3}.\\text{yr}^{-1}$). \n",
    "Because there are so many possible detectors, we take only 1 observing run of 3 waveform set repetitions! The code in the cell immediately below was duplicated 3 times each for merger rates of $295.7\\text{Gpc}^{-3}.\\text{yr}^{-1}$ (``high``), $105.5\\text{Gpc}^{-3}.\\text{yr}^{-1}$ (``mid``), $21.6\\text{Gpc}^{-3}.\\text{yr}^{-1}$ (``low``) according to [Abbott et al. 2022] and was run simultaneously using separate python kernels (WARNING: very CPU-intensive!) i.e. 3 observing runs x 3 waveform set repetitions for each merger rate. \n",
    "\n",
    "**Instructions**: Change the ``curves_path`` variable to the directory with the ASD text files. Copy and paste code into new notebooks to run searches simultaneously. Create a folder in parent directory to store numpy array and replace the ``run_path`` variable respectively. Change the ``save_name`` variable for the filename (e.g. ``search_mid_1`` means the ``mid`` merger rate and the 1st search). \n",
    "\n",
    "**TODO**: \n",
    "\n",
    "1. Speed-up the grid search using multi-processing in python e.g. https://github.com/johanneseder711/Parallelization for Mac (current runs take nearly 24h).  \n",
    "\n",
    "2. Scrutinise the BNS merger rate more closely in the literature.\n",
    "\n",
    "3. Explore different search algorithms e.g. particle swarm.\n",
    "\n",
    "4. Investigate the distance randomisation of BNS mergers (currently calculated using ``merger_rates_for_mallika.py``) and the artefacts of binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306f684a-beb0-4178-a764-8a4f25cda582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement BNS_Optimisation_Module_Randomised (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for BNS_Optimisation_Module_Randomised\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install BNS_Optimisation_Module_Randomised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a8730d-4dad-419c-a651-121f52ef1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nemo_optimisation_modules as nom\n",
    "# import numpy as np\n",
    "# import BNS_Optimisation_Module_Randomised as bnso #import module\n",
    "# import bilby\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import gwinc\n",
    "# import astropy.units as u\n",
    "# import astropy.constants as c\n",
    "# from scipy.interpolate import interp1d\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from csv import writer\n",
    "# import pandas as pd\n",
    "# import time as t\n",
    "\n",
    "# df, index = bnso.sim_index(sim_data_name = 'core_database_index_grid_spacing_min_removefpeak1.5to4.csv');     \n",
    "# h5_name = \"NR_base_final.hdf5\"\n",
    "# bnso.make_h5(h5_name);\n",
    "# duration, sampling_frequency, injection_parameters = bnso.initial_parameters(); \n",
    "# for i in index: #50% sure putting a for loop inside a function might be a bad idea so setup the for loop to loop over the\n",
    "#         #NR simulations here\n",
    "#     bnso.collect(i, df, duration, sampling_frequency,injection_parameters, h5_name ,None); #collect the data for each simulation\n",
    "#         #and append to the h5 file\n",
    "\n",
    "# detrate_ndarr = np.zeros((10,10,10,10))\n",
    "# fsig = np.geomspace(100,10e3,201)\n",
    "# curves_path = './FinalSensitivityCurvesSqueezed/'\n",
    "# run_path = './ObservingRuns/'\n",
    "# IFO_files = os.listdir(curves_path)\n",
    "# detector_horizon = 400*u.Mpc\n",
    "# merger_rate = 105.5\n",
    "# save_name = 'search_mid_1.npy'\n",
    "\n",
    "# start_time = t.time()\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         for k in range(10):\n",
    "#             for l in range(10):\n",
    "#                 match_ASD = [ifo for ifo in IFO_files if f\"{i},{j},{k},{l}\" in ifo]\n",
    "#                 if len(match_ASD) > 1:\n",
    "#                     print(f\"Found a file with non-unique file! Check index: {i},{j},{k},{l}\")\n",
    "#                 ASD_path = match_ASD[0]\n",
    "#                 ASDarr = np.array([float(line.rstrip()) for line in open(curves_path+ASD_path, 'r')])\n",
    "#                 PMS_filter = [i for i in range(len(fsig)) if fsig[i]>2e3 and fsig[i]<4e3]\n",
    "#                 ASD_filtered = ASDarr[PMS_filter]\n",
    "#                 if all([asd > 1e-20 for asd in ASD_filtered]):\n",
    "#                     continue\n",
    "#                 IFO, name = bnso.IFOmakerFromASDArray('Config_1',duration,sampling_frequency, 0., fsig, ASDarr); #define the IFO\n",
    "#                 detratelist = []\n",
    "#                 observing_runs = 1\n",
    "#                 for m in range(0,observing_runs): #number of rates calculated\n",
    "#                     repeat_waveforms = 3 #number of observing_times you want the full waveform set repeated\n",
    "#                     total_events = list(index)*repeat_waveforms #creates an index where each number is repeated repeat_waveforms amount of observing_times\n",
    "#                     np.random.shuffle(total_events) #shuffles the total_events for each loop\n",
    "#                     observing_time, random_param = bnso.random_param(df,detector_horizon,merger_rate, scalewavno=repeat_waveforms); #creates set of random injection params\n",
    "#                         #for the merger event rate and the corresponding scaling observing_time used to get the number of waveforms\n",
    "#                     SNRlist = []\n",
    "#                     for n in range(0,len(total_events)): #this creates the list of SNRs for all the injection parameters\n",
    "#                         AusIFO = IFO;\n",
    "#                         injection_parameters = dict(distance=random_param['distance'][n], phase=random_param['phase'][n], ra=random_param['ra'][n], \n",
    "#                                                 dec=random_param['dec'][n], psi=random_param['psi'][n], t0=0., geocent_time=0.)\n",
    "#                         SNR = bnso.calc_SNR(total_events[n], duration,sampling_frequency, injection_parameters,AusIFO,df);\n",
    "#                         SNRlist.append(SNR)\n",
    "#                     detratescaled = sum(n > 5 for n in SNRlist) #finds the number of entries in the SNR list above thresshold\n",
    "#                     detrate = detratescaled/observing_time #rescales to find the detrate per year\n",
    "#                     detratelist.append(detrate)\n",
    "#                 detrate_ndarr[i,j,k,l] = np.mean(detratelist)\n",
    "# end_time = t.time()\n",
    "# np.save(run_path+save_name,detrate_ndarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de207ec0-eb36-48a2-bd05-2042a5032e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BNS_Optimisation_Module_Randomised'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnemo_optimisation_modules\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnom\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBNS_Optimisation_Module_Randomised\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnso\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BNS_Optimisation_Module_Randomised'"
     ]
    }
   ],
   "source": [
    "import nemo_optimisation_modules as nom\n",
    "import numpy as np\n",
    "import BNS_Optimisation_Module_Randomised as bnso\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Initialize parameters\n",
    "curves_path = './FinalSensitivityCurvesSqueezed/'\n",
    "run_path = './ObservingRuns/'\n",
    "IFO_files = os.listdir(curves_path)\n",
    "fsig = np.geomspace(100, 10e3, 201)\n",
    "PMS_filter = (fsig > 2e3) & (fsig < 4e3)\n",
    "detector_horizon = 400 * u.Mpc\n",
    "merger_rate = 105.5\n",
    "save_name = 'search_mid_1.npy'\n",
    "duration, sampling_frequency, injection_parameters = bnso.initial_parameters()\n",
    "\n",
    "# Map ASD files to indices for quick access\n",
    "asd_file_map = {}\n",
    "for file in IFO_files:\n",
    "    indices = tuple(map(int, file.rstrip(\".txt\").split(',')))\n",
    "    asd_file_map[indices] = file\n",
    "\n",
    "# Define the main calculation for parallelization\n",
    "def calculate_detrate(i, j, k, l):\n",
    "    if (i, j, k, l) not in asd_file_map:\n",
    "        return None  # Skip if file not found for these indices\n",
    "    \n",
    "    ASD_path = curves_path + asd_file_map[(i, j, k, l)]\n",
    "    ASDarr = np.array([float(line.strip()) for line in open(ASD_path, 'r')])[PMS_filter]\n",
    "    if np.all(ASDarr > 1e-20):\n",
    "        return None  # Skip if all ASD_filtered values exceed threshold\n",
    "    \n",
    "    # Prepare IFO and detection rates\n",
    "    IFO, _ = bnso.IFOmakerFromASDArray('Config_1', duration, sampling_frequency, 0., fsig, ASDarr)\n",
    "    detratelist = []\n",
    "    observing_runs = 1\n",
    "\n",
    "    for _ in range(observing_runs):\n",
    "        repeat_waveforms = 3\n",
    "        total_events = list(index) * repeat_waveforms\n",
    "        np.random.shuffle(total_events)\n",
    "        observing_time, random_param = bnso.random_param(df, detector_horizon, merger_rate, scalewavno=repeat_waveforms)\n",
    "\n",
    "        SNRlist = []\n",
    "        for n in total_events:\n",
    "            injection_params = {\n",
    "                'distance': random_param['distance'][n],\n",
    "                'phase': random_param['phase'][n],\n",
    "                'ra': random_param['ra'][n],\n",
    "                'dec': random_param['dec'][n],\n",
    "                'psi': random_param['psi'][n],\n",
    "                't0': 0.,\n",
    "                'geocent_time': 0.\n",
    "            }\n",
    "            SNR = bnso.calc_SNR(n, duration, sampling_frequency, injection_params, IFO, df)\n",
    "            SNRlist.append(SNR)\n",
    "\n",
    "        detrate = sum(snr > 5 for snr in SNRlist) / observing_time\n",
    "        detratelist.append(detrate)\n",
    "    \n",
    "    return (i, j, k, l, np.mean(detratelist))\n",
    "\n",
    "# Run in parallel and collect results\n",
    "start_time = t.time()\n",
    "detrate_ndarr = np.zeros((10, 10, 10, 10))\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(calculate_detrate, i, j, k, l) for i in range(10) for j in range(10) for k in range(10) for l in range(10)]\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            i, j, k, l, mean_detrate = result\n",
    "            detrate_ndarr[i, j, k, l] = mean_detrate\n",
    "\n",
    "end_time = t.time()\n",
    "np.save(run_path + save_name, detrate_ndarr)\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85907115-2251-4a51-ae62-5b5115e6d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
